<!DOCTYPE.HTML>
<html>
<head>
<title>Moore's Law and the AI Apocalypse</title>
<link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet" type="bootstrap"/>
  <link type="text/css" rel="stylesheet" href="stylesheet.css"/>
  <link type="text/css" rel="stylesheet" href="stylesheet.css"/>
  <style>
    .banner {
 background: url(https://ae01.alicdn.com/kf/HTB1tBJNLpXXXXaPXpXXq6xXFXXXV/alternative-art-digital-art-colorful-font-b-paint-b-font-font-b-splatter-b-font-font.jpg) no-repeat center center fixed;
 background-size: cover;
 height: 70px;
 color: white;
 text-align: left;
 position: fixed;
 width: 100%;
 z-index: 999;
 margin-top: -100px;
  
}
 
.nav {
  background: black;
  height: 40px;
  width: 100%;
  position: fixed;
  margin-top: -30px;
  z-index: 998;
     box-shadow:  5px 5px 30px #888888;

}
.nav ul {
  list-style: none;
  margin: 0 auto; 
  padding: 0;
  height: 10%;
}
.nav ul li {
  display: inline-block;
  color: white;
  height: 30px;
  line-height: 30px;
  list-style: none;
  padding: 0 10px;
  transition: background .5s;
  text-align: center;
  font-family: sans-serif;
  margin-right: 30px;
 margin-top: 5px;
 border-radius: 5px;
}

.nav ul li:hover {
  background: #FF9D00;
  cursor: pointer; 
  text-decoration: none;
  font-color: black;
}

.nav a {
    
 text-decoration: none;
 color: white;
 font-weight: bold;
}

  form {
    display: inline;
    margin-left: 100px;
  }
body {
  background-color: #D4FFF1;
}


h2{
  font-size: 40px;
  margin-top: -60px;
  line-height: 50px; 
  color: white;
  background: #1C2733;
  margin-left: 30px;
  text-decoration: none;

}

h3 {

	margin-top: 0px;
}

h3, h4, h5 {
	  margin-left: 30px;
}

 .footer {
  background-color: black;
  height: 40px;

  width: 100%;
     box-shadow:  5px 5px 30px #888888;

}
.footer ul {
  list-style: none;
  padding: 0;
  height: 10%;
}
.footer ul li {
  display: inline-block;
  color: white;
  height: 30px;
  line-height: 30px;
  list-style: none;
  padding: 0 10px;
  transition: background .5s;
  text-align: center;
  font-family: sans-serif;
  margin-right: 30px;
 margin-top: 5px;
 border-radius: 5px;
}

.footer ul li:hover {
  background: #FF9D00;
  cursor: pointer; 
  text-decoration: none;
  font-color: black;
}

.footer a {
    
 text-decoration: none;
 color: white;
 font-weight: bold;
}
 .title {
 	
 	margin-left: 30px;
 	border-bottom: 5px solid black;
 	width: 80%;
 	padding-bottom: 10px;

 }
.main {


}

.sidebar {
    margin: auto;
    float: right;
    width: 10%;
    margin-left: 30px;
    z-index: 0;
    margin-right: 1%;
    overflow: scroll;
            box-shadow:  5px 5px 30px #888888;
  

}

.book {
  background-color: white;
  border: 1px solid black;
    margin-bottom: 10px;
            box-shadow:  5px 5px 30px #888888;
}
  .sidebar h1{
    font-size: 20px;
    text-overflow: wrap;
  }

  .sidebar h2 {
font-size: 15px;
  }

  .sidebar h3 {
    font-size: 12px;

  }

  .sidebar p{
font-size: 10px;
  }

img
 {
	border-radius: 100;
	margin-left: 0px;
	height: 50px;
}

.jumbotron {
  background: url("http://images.contentful.com/7h71s48744nc/3l9IwugJKgcCq8YMwWGiy/8fdbc22b56ca7a2d34fb26e76952533e/i-robot.jpg")no-repeat center;
 background-size: cover;
  height: 400px;
  text-align: center;
  font-color: white;
  margin-top: 100px;
  box-shadow:  5px 5px 30px #888888;

  }

 .jumbotron h1 {
  color: white;
 }
.col-sm-1 {
	
	padding-left: -100px;
}

p {
	margin-bottom: -5px;
	margin-top: 5px;
	margin-left: -20px;
	}

.main p {
	font-size: 18px;
	line-height: 40px;
	margin-left: 30px;
	width: 80%;
	text-indent: 3em;
	margin-top: 30px;
}
.source {
	background-color: white;
	margin-bottom: -10;
	border-top: 5px solid #1C2733;
}

h6 {
	font-size: 25px;

}

ul {
padding-bottom: 30px;
}

li {
	font-size: 15px;
	padding-bottom: 10px;
}

</style>


</head>
<body>
<div class="banner"><h1>CAT'S CRADLE (OF SCIENCE)</h1></div>
<div class="nav">
	<div class="container">
  <ul>
    <li><a href="home.html">Home</a></li>
    <li><a href="aboutus.html">About Us</a></li>
    <li><a href="index.html">Articles</a></li>
    <li><a href="mars.html">Space Exploration</a></li>
  </ul>
  </div>
  </div>
 <div class="jumbotron">
 </div>
 
          <h2>Moore's Law and the AI Apocalypse</h2>
          <h3>ARTIFICIAL INTELLIGENCE</h3>
         <div class="title"> <div class="row"><div class="col-sm-1"><img src="Screen Shot 2017-11-03 at 1.01.57 PM.png"><a href="//imgur.com/HYf6n"></a></blockquote><script async src="//s.imgur.com/min/embed.js" charset="utf-8"></script></div><div class="col-sm-2"><p>By <a href="aboutus.html">Lucy Yang</a><p>November 3rd, 2017</p>
          
          </div></div>
 </div>
 <div class="sidebar">
 </div>
 <div class="main">
<p> Since the creation of the atomic bomb, technological advancement has faced strong social and political backlash. First of all, weapons of mass destruction put lives at risk, so people should have a say about a technology that may impact their well-being. Technological advancements in biology and chemistry combined with nanotechnology can change genes, the innate traits of each individual. Worst of all, the progress in technology always involves unexpected consequences ranging from creating more inequality to generating monsters like Frankenstein--and once the genie is out of the bottle, it becomes impossible to put it back in. Technology’s irreversible characteristic terrifies ordinary people as well as leading scientists. When AlphaGo, an artificial intelligence (AI) program designed to play the chess-like board game Go, defeated the legendary champion Lee Sedo, AI technology had clearly crossed a significant threshold. This news shocked the world because for the first time, AI had surpassed human intelligence, at least in this particular way. Technology is supposed to enhance human capabilities, but such developments may result in technology becoming our own worst enemy. The potential implications of AI and other advanced technologies include serious threats to human well-being and the social order, as well as various ethical dilemmas. Given the exponential learning capabilities of AI, such agents may become autonomous, decide to collude with each other and oppose human society as a whole. If AI agents acquire self-consciousness, they may demand human rights and even greater privileges than those afforded to most people. Will AI agents be able and expected to exhibit moral behavior? How might we define a moral AI entity? Will humans remain capable of controlling AI agents?
</p>
<p>  What human beings fear the most are machines like us—with intelligence, morality, and emotion—because we want to keep machines to a degraded status, where they simply receive orders and perform the tasks. I’ll start with how fast the technology develops and whether Frankenstein can become a reality. However, I’m making the assertion that only by programming morality into AI agents, the human race can be kept from technological threats. If a machine has morality, able to distinguish evil from virtue, it will not be subjective to immoral orders. Instead, they will become independent thinker like humans, and the only difference would be their greater capacity with unlimited database and higher-level computational skills.  Finally, there are some potential ways, which scientists have been researching on for years, to turn AI agents into independent thinkers.</p>
<h4>Moore’s Law and the AI Apocalypse</h4>
<p> Moore’s Law, which predicts the exponential growth in technological advancement, is probably the best method to date for characterizing the speed of computational AI agents. The law initially only dealt with what would happen over the next decade in a semiconductor components industry, with Moore predicting that the capacity of such semiconductor would experience an exponential growth from 1965 to 1975 (Schaller). Moore’s prediction that a six-millimeter silicon chip would be able to contain 65000 components by 1975 was based on only three data points, but it comes out to be right on the mark. 
</p>
<p>    Since its inception, Moore’s Law has gradually evolved from a specified predictor of the semiconductor industry to an almost-universal predictor of the whole technology industry, applying to many real life cases. For example, back in the eighties, the most popular computer model Osborne I weighs twenty-three pounds but only has a 64 KB storage capability. However, three decades later, Macbooks weight only 2.3 pounds but has a storage capability of 128 GB, indicating a more than one million times growth. The rapid decrease in weights but the increase in capability proves Moore’s law to be accurate. Because of the exponential amplification of AI, software agents or other computational entities may be able to perform tasks that we consider infeasible now, just as they are able to perform tasks today that were infeasible in the past. A hundred years ago, it was hard to imagine the roles that computers would take on—Siri, for example.</p>
<p>    Yet, what scares people is that, as Moore himself stated, the exponential law gradually becomes a self-fulfilling prophecy, meaning that Moore’s Law may more accurately represent a prescription than a description for computational development. Once the law was established, researchers set the exponential growth as a goal and try their best to achieve it without giving any thought to potentially destructive consequences. What is more horrific than exponential expansion is the fact that once a technology is developed, it is impossible to take back, and weapons of mass destruction are the best illustration. During the cold war, the government granted nuclear projects a substantial amount of funding—The Manhattan Project alone received more than 2 billion dollars—like today the government and major corporations distribute billions of dollars to AI development. In 2008, X Prize founder Peter Diamandis and Google director of engineering Ray Kurzweil even founded a Singularity University in Silicon Valley, with the goal of using exponential technologies to address the most intractable problems. All Singularity University’s partner companies targeted on using artificial intelligence, digital body, nanotechnology, and robotics to deal with health, environment, security and other social issues. Although the majority of people now recognized that nuclear weapons put innocent lives at risk, such technology cannot be called back. A large number of politicians insisted that nuclear weapons serve as a deterrent, preventing countries from involving another world war. However, what if the weapons fall under the control of unstable regimes or extremist governments? The U.S. invaded Iraq with the assertion that Saddam Hussein possesses weapons of mass destruction. The statement is false, but the fact that terrorist groups have been trying to buy weapons of mass destruction cannot be denied. International regulations, such as the Partial Test Ban and the Nuclear Non-Proliferation Treaty, failed to calm fears of being bombed, which could never go away since nuclear bomb’s first use. Similarly, once we reach the technological singularity, a state in which artificial intelligence surpasses human intelligence, humans would not be able to retrieve robots and AI agents who can think and act autonomously. </p>
<p>   Understanding the irreversibility of technology, many leading theoretical scientists reveal serious concerns about the devastating consequences of having AI agents, and worst of all is the thought of AI apocalypse. In his article, “Apocalypse,” Phil Torres examines how emerging technology will fuel apocalyptic terrorism in the future (Torres). The history of terrorism began with a religious worldview, evolved to nationalism, and eventually embraced religious extremism. Recently, the deadliest terrorist group, ISIS, has begun to put more emphasis on eschatology—a theory concerned with death, judgment, and the final destiny of mankind—than any former terrorist groups. And given the technological capabilities that the GNR(genetics, nanotechnology, and robotics)  revolution represents as a whole, such apocalyptic terrorism could become a reality. </p>
<p>   A more serious situation might include the prospect of AI agents becoming autonomous and thus beyond human control. They may become an entirely different species, at once intellectually and physically superior to human beings. As Stephen Hawking warned, “the development of full artificial intelligence could spell the end of human race” (Cellan-Jones). To illustrate the dangers, Hawking drew an analogy between AI systems and alien civilizations (Luckerson). If aliens sent us a message saying, “We’ll arrive in a few decades,” we would not merely reply, “OK, just send us a note when you arrive.” Instead, human beings would immediately begin to undertake actions and to implement precautions to prevent the aliens from successfully landing, or at least for protecting ourselves in the case of a landing. Similarly, robots a new species, but because they are created and owned by humans so far, we do not perceive them as aliens, thus threats. Nevertheless, we should consider the statements of leading scientists as just such a note from AI, and respond accordingly. </p>
<p> Another prominent theorist Nick Bostrom, director of the Future of Human Institute at the University of Oxford, seemly argued that once singularity is reached, machines could mobilize and decide to eradicate human extremely quickly. Most leading opponents of AI believe that in order to guarantee safety, we need to tear ourselves away from technology because it is no longer a tool to intention but a machine that is capable of human behaviors (Latour). However, technology makes life easier and could help humans solve almost any kind of problem—disease, pollution, accidents, and other unnecessary sufferings—so it is impossible and unwise for humans to just abandon technology. Moreover, it is already hard to distinguish a non-technique machine from a technique one simply because life and technology are a coherent body now. It will become even harder to perceive the difference in the future as human-machine hybrids may be developed. Therefore, I’m here to argue that we not only need to continue investing in AI but also to develop strong AI that has morality and emotions. </p>
</div>
<div class="source">
	<ul><h6>Courtesy Of:</h6>
		<li>Torres, Phil. "Agential Risks: A Comprehensive Introduction." Journal of Evolution and Technology. N.p., Aug. 2016. Web. 17 July 2017. </li>
    <li>Schaller, Robert R. "Moore's Law: Past, Present, and Future." Moore's Law: Past, Present and Future. Linda Geppert, n.d. Web. 17 July 2017.</li>
    <li>"AlphaGo." DeepMind. N.p., n.d. Web. 18 July 2017.
</li>
    <li>Latour, Bruno. "Morality and Technology: The End of the Means." Trans. Couze Venn. Sage Journals. N.p., 1 Dec. 2002. Web. 10 July 2017.
/.latest_citation_text</li>
<li>Luckerson, Victor. "Stephen Hawking Says Artificial Intelligence Could End Human Race." Time. Time, n.d. Web. 29 July 2017.</li>
<li>Cellan-Jones, Rory. "Stephen Hawking Warns Artificial Intelligence Could End Mankind." BBC News. BBC, 02 Dec. 2014. Web. 19 July 2017.</li>
	</ul>
</div>
<div class="footer">
  <ul>
  </ul>
  </div>
</body>
</html>
